{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from util import get_transforms, get_dataset, get_image_size, get_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy\n",
    "\n",
    "\n",
    "dataset_name = 'MNIST'\n",
    "\n",
    "batch_size = 1  # record each loss element, not mean\n",
    "timesteps = 300\n",
    "\n",
    "image_size = get_image_size(dataset_name)\n",
    "transform, _ = get_transforms(image_size=image_size)\n",
    "trainset, testset = get_dataset(dataset_name, transform)\n",
    "trainloader, testloader = get_dataloader(trainset, testset, 1)\n",
    "\n",
    "def flatten_features(record_latent_Features):\n",
    "    for key in record_latent_Features.keys():\n",
    "        for i in range(len(record_latent_Features[key])):\n",
    "            record_latent_Features[key][i] = record_latent_Features[key][i].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify UNet \n",
    "add 3 functions to the UNet class\n",
    "- _record_latent_features()\n",
    "- _stop_record_latent_features()\n",
    "- _start_ood_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, label):\n",
    "        super().__init__()\n",
    "\n",
    "        if label:\n",
    "            self.label_embedding = nn.Embedding(1, 8)\n",
    "            self.label_mlp = nn.Linear(1, out_channels)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Dropout2d(0.03),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, label=None):\n",
    "        # print(x.shape)\n",
    "        h = self.conv1(x)\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(...,) + (None,) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        if label:\n",
    "            label_emb = self.relu(self.label_mlp(label))\n",
    "            label_emb = label_emb[(...,) + (None,) * 2]\n",
    "            h = h + label_emb\n",
    "        # Second Conv\n",
    "        h = self.conv2(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_channels=3,\n",
    "        channels=(64, 128, 256, 512),\n",
    "        time_emb_dim=32,\n",
    "        label=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # latent recording\n",
    "        self.record_latent = False # activate only when we call _record_latent_features()\n",
    "        # self.record_timesteps = record_timesteps\n",
    "        # if self.record_latent:\n",
    "        #     self.record_latent_features = { key:[] for key in record_timesteps }\n",
    "        self.ood_detection_indicator = False\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList()\n",
    "        for channel in channels:\n",
    "            self.downs.append(Block(input_channels, channel, time_emb_dim, label))\n",
    "            input_channels = channel\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = Block(channels[-1], channels[-1] * 2, time_emb_dim, label)\n",
    "\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList()\n",
    "        for channel in reversed(channels):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(channel * 2, channel, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(Block(channel * 2, channel, time_emb_dim, label))\n",
    "\n",
    "        self.output = nn.Conv2d(channels[0], output_channels, kernel_size=1)\n",
    "\n",
    "    def _record_latent_features(self, record_timesteps):\n",
    "        # latent recording\n",
    "        self.record_latent = True\n",
    "        self.record_timesteps = record_timesteps\n",
    "        if self.record_latent:\n",
    "            self.record_latent_features = { key:[] for key in record_timesteps }\n",
    "\n",
    "    def _stop_record_latent_features(self):\n",
    "        self.record_latent = False\n",
    "\n",
    "    def _start_ood_detection(self, ood_detector, detect_timesteps):\n",
    "        self.detect_timesteps = detect_timesteps\n",
    "        self.ood_detection_indicator = True\n",
    "        self.ood_detector = ood_detector\n",
    "        self.ood_detect_res = []\n",
    "\n",
    "    def forward(self, x, timestep, label=None):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t, label)\n",
    "            residual_inputs.append(x)\n",
    "            x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        # print(timestep)\n",
    "        # record the bottleneck latent features to detect OOD samples (only in training mode)\n",
    "        # only record a few timesteps, since first few steps already contain enough information\n",
    "        if self.record_latent:\n",
    "            for i, _t in enumerate(timestep):\n",
    "                if _t in self.record_timesteps:\n",
    "                    self.record_latent_features[_t.item()].append(x[i].detach().cpu().numpy())\n",
    "        \n",
    "        if self.ood_detection_indicator:\n",
    "            for i, _t in enumerate(timestep):\n",
    "                if _t in self.detect_timesteps:\n",
    "                    # print(x[i].detach().cpu().numpy().reshape(1,-1).shape)\n",
    "                    ood_pred, max_dist = self.ood_detector.detect_l2_distance_ood(x[i].detach().cpu().numpy().reshape(1,-1), _t.item())\n",
    "                    self.ood_detect_res.append((ood_pred, max_dist))\n",
    "\n",
    "        x = self.bottleneck(x, t)\n",
    "        for i in range(0, len(self.ups), 2):\n",
    "            conv_t = self.ups[i]\n",
    "            up = self.ups[i + 1]\n",
    "            residual_x = residual_inputs.pop()\n",
    "\n",
    "            x = conv_t(x)\n",
    "\n",
    "            if x.shape != residual_x.shape:\n",
    "                x = TF.resize(x, size=residual_x.shape[2:], antialias=True)\n",
    "\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t, label)\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the DDPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from unet import UNet\n",
    "from model import DiffusionModel\n",
    "from util import get_transforms, get_dataset, get_image_size, get_dataloader\n",
    "import sys\n",
    "\n",
    "\n",
    "def run_latent_all_samples(unet, loss_fn, trainloader, diffusion_model, device, batch_size, record_timesteps):\n",
    "    unet.eval()\n",
    "    related_loss = { key:[] for key in record_timesteps }\n",
    "    for timestep in record_timesteps:\n",
    "        for batch, _ in trainloader:\n",
    "            t = torch.full((batch_size, ), timestep).to(device)\n",
    "            batch = batch.to(device)\n",
    "            batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
    "            predicted_noise = unet(batch_noisy, t)\n",
    "            loss = loss_fn(noise, predicted_noise)\n",
    "            related_loss[timestep].append(loss.item())\n",
    "            print(f'length of record_latent_features: {np.sum([len(unet.record_latent_features[key]) for key in unet.record_latent_features.keys()])}')\n",
    "\n",
    "    torch.save(unet.record_latent_features, \"weight/record_latent_features.pt\")\n",
    "    torch.save(related_loss, \"weight/record_latent_features_loss.pt\")\n",
    "\n",
    "    print(f'length of record_latent_features: {np.sum([len(unet.record_latent_features[key]) for key in unet.record_latent_features.keys()])}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_name = 'MNIST'\n",
    "    batch_size = 1  # record each loss element, not mean\n",
    "    timesteps = 300\n",
    "\n",
    "    image_size = get_image_size(dataset_name)\n",
    "    transform, _ = get_transforms(image_size=image_size)\n",
    "    trainset, testset = get_dataset(dataset_name, transform)\n",
    "    # trainloader, testloader = get_dataloader(trainset, testset, batch_size)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=8, drop_last=True)\n",
    "\n",
    "    record_timesteps = (0,1,5,10,25,50,100,200,299)\n",
    "    unet = UNet(input_channels=1, output_channels=1, record_latent=True).to('cuda')\n",
    "    unet.load_state_dict(torch.load('weight/parameters_power_mnist.pkl'))\n",
    "    diffusion_model = DiffusionModel(timesteps=timesteps)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    unet._record_latent_features(record_timesteps=record_timesteps)\n",
    "    run_latent_all_samples(unet, loss_fn, trainloader, diffusion_model, 'cuda', batch_size, record_timesteps)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the stored features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_latent_features = torch.load('./weight/record_latent_features.pt')\n",
    "record_latent_features_loss = torch.load('./weight/record_latent_features_loss.pt')\n",
    "flatten_features(record_latent_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the OOD Detector\n",
    "Only two methods sofar\n",
    "- distance-based method\n",
    "- norm-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_Phi(Phi,num_k, method):\n",
    "    # the rows of vh are the eigenvectors of Phi^HPhi, \n",
    "    # i.e. the principal components of Phi\n",
    "    if method == 'np':\n",
    "        U, S, Vh = np.linalg.svd(Phi)\n",
    "        print(U.shape, S.shape, Vh.shape)\n",
    "        # select the top-k principal components\n",
    "        topk_pc = Vh[:num_k,:]\n",
    "        # project Phi to top-k principal components\n",
    "        Phi_projected = Phi @ topk_pc.T\n",
    "        print(Phi_projected.shape)\n",
    "    elif method == 'scipy':\n",
    "        U, S, Vh = scipy.linalg.svd(Phi, full_matrices=False)\n",
    "        print(U.shape, S.shape, Vh.shape)\n",
    "        # select the top-k principal components\n",
    "        topk_pc = Vh[:num_k,:]\n",
    "        # project Phi to top-k principal components\n",
    "        Phi_projected = Phi @ topk_pc.T\n",
    "        print(Phi_projected.shape)\n",
    "    else:\n",
    "        raise ValueError(\"no such svd method\")\n",
    "    return Phi_projected, (U,S,Vh)\n",
    "\n",
    "\n",
    "def lower_triangular_flatten(matrix):\n",
    "    # Extract the lower triangular part using np.tril_indices\n",
    "    lower_triangular_indices = np.tril_indices(matrix.shape[0])\n",
    "    \n",
    "    # Use the indices to extract the lower triangular elements\n",
    "    lower_triangular_elements = matrix[lower_triangular_indices]\n",
    "\n",
    "    return lower_triangular_elements\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class pca_ood_detector:\n",
    "    def __init__(self, record_latent_features):\n",
    "        \n",
    "        self.record_latent_features = record_latent_features\n",
    "        self.timesteps = [key for key in self.record_latent_features]\n",
    "\n",
    "    def flatten_features(self):\n",
    "        for key in self.record_latent_features.keys():\n",
    "            for i in range(len(self.record_latent_features[key])):\n",
    "                self.record_latent_features[key][i] = self.record_latent_features[key][i].flatten()\n",
    "\n",
    "    def pca_analyze(self, topk=100):\n",
    "\n",
    "        self.pca_basis = { key:None for key in self.record_latent_features.keys() }\n",
    "        self.pca_projected_latents = { key:None for key in self.record_latent_features.keys() }\n",
    "        tqmdr = tqdm(self.record_latent_features.keys(), desc='pca_analysis')\n",
    "\n",
    "        for key in tqmdr:\n",
    "            U, S, Vh = scipy.linalg.svd(self.record_latent_features[key], full_matrices=False)\n",
    "            print(U.shape, S.shape, Vh.shape)\n",
    "            # select the top-k principal components\n",
    "            topk_pc = Vh[:topk,:]\n",
    "            # project Phi to top-k principal components\n",
    "            Phi_projected = self.record_latent_features[key] @ topk_pc.T\n",
    "\n",
    "            self.pca_basis[key] = topk_pc\n",
    "            self.pca_projected_latents[key] = Phi_projected\n",
    "\n",
    "    def calc_avg_distance(self):    # setting threshold\n",
    "\n",
    "        pca_projected_latents_dist = {}\n",
    "        pca_projected_latents_stats = {}\n",
    "\n",
    "        tqdmr = tqdm(self.pca_projected_latents.keys(), desc='calculate the stats of latent features for training samples')\n",
    "\n",
    "        for key  in tqdmr:\n",
    "\n",
    "            rand_ids = np.random.choice(len(self.pca_projected_latents[key]), size=10000, replace=False)\n",
    "            vectors = np.array(self.pca_projected_latents[key])[rand_ids,:]\n",
    "            \n",
    "            # Compute squared distances\n",
    "            dot_product = np.dot(vectors, vectors.T)\n",
    "            squared_norms = np.sum(vectors**2, axis=1, keepdims=True)\n",
    "            squared_distances = squared_norms + squared_norms.T - 2 * dot_product\n",
    "            \n",
    "            # Ensure distances are non-negative\n",
    "            squared_distances = np.maximum(squared_distances, 0)\n",
    "            \n",
    "            # Take the square root to get L2 distance\n",
    "            distances = np.sqrt(squared_distances)\n",
    "\n",
    "            pca_projected_latents_dist[key] = distances\n",
    "            pca_projected_latents_stats[key] = (np.mean(distances), np.std(lower_triangular_flatten(distances)))\n",
    "\n",
    "        self.pca_projected_latents_dist = pca_projected_latents_dist\n",
    "        self.pca_projected_latents_stats = pca_projected_latents_stats\n",
    "\n",
    "    def set_threshold(self, sigma_threshold=5): # mean+3*std\n",
    "\n",
    "        self.thresholds = { key:self.pca_projected_latents_stats[key][0]+sigma_threshold*self.pca_projected_latents_stats[key][1] \n",
    "                          for key in self.pca_projected_latents_stats.keys() }\n",
    "\n",
    "    def detect_l2_distance_ood(self, latent, timestep):\n",
    "        assert timestep in self.timesteps \n",
    "\n",
    "        projected_latent = latent @ self.pca_basis[timestep].T\n",
    "        # projected_latent = self.pca_models[timestep].transform(latent)\n",
    "        l2_distances = np.linalg.norm(self.pca_projected_latents[timestep] - projected_latent, axis=1)\n",
    "        min_distance = np.min(l2_distances)\n",
    "        if min_distance > self.thresholds[timestep]:\n",
    "            print('ood sample!')\n",
    "            return True, min_distance\n",
    "        else:\n",
    "            print('id sample!')\n",
    "            return False, min_distance\n",
    "\n",
    "\n",
    "class pca_ood_detector_norm:\n",
    "    def __init__(self, record_latent_features):\n",
    "        \n",
    "        self.record_latent_features = record_latent_features\n",
    "        self.timesteps = [key for key in self.record_latent_features]\n",
    "\n",
    "    def flatten_features(self):\n",
    "        for key in self.record_latent_features.keys():\n",
    "            for i in range(len(self.record_latent_features[key])):\n",
    "                self.record_latent_features[key][i] = self.record_latent_features[key][i].flatten()\n",
    "\n",
    "    def pca_analyze(self, topk=100):\n",
    "\n",
    "        self.pca_basis = { key:None for key in self.record_latent_features.keys() }\n",
    "        self.pca_projected_latents = { key:None for key in self.record_latent_features.keys() }\n",
    "        tqmdr = tqdm(self.record_latent_features.keys(), desc='pca_analysis')\n",
    "\n",
    "        for key in tqmdr:\n",
    "            U, S, Vh = scipy.linalg.svd(self.record_latent_features[key], full_matrices=False)\n",
    "            print(U.shape, S.shape, Vh.shape)\n",
    "            # select the top-k principal components\n",
    "            topk_pc = Vh[:topk,:]\n",
    "            # project Phi to top-k principal components\n",
    "            Phi_projected = self.record_latent_features[key] @ topk_pc.T\n",
    "\n",
    "            self.pca_basis[key] = topk_pc\n",
    "            self.pca_projected_latents[key] = Phi_projected\n",
    "\n",
    "    def calc_avg_norm(self):    # setting threshold\n",
    "        pca_projected_latents_norm = {}\n",
    "        pca_projected_latents_norm_stats = {}\n",
    "\n",
    "        tqdmr = tqdm(self.pca_projected_latents.keys(), desc='calculate the stats of latent features for training samples')\n",
    "\n",
    "        for key in tqdmr:\n",
    "            norms = np.linalg.norm(self.pca_projected_latents[key],axis=1)\n",
    "            pca_projected_latents_norm[key] = norms\n",
    "            pca_projected_latents_norm_stats[key] = (np.mean(norms), np.std(norms)) \n",
    "           \n",
    "        self.pca_projected_latents_norm = pca_projected_latents_norm\n",
    "        self.pca_projected_latents_norm_stats = pca_projected_latents_norm_stats\n",
    "\n",
    "    def set_threshold(self, sigma_threshold=5): # mean+3*std\n",
    "\n",
    "        self.thresholds = { key:(self.pca_projected_latents_norm_stats[key][0]+sigma_threshold*self.pca_projected_latents_norm_stats[key][1] ,\n",
    "                                self.pca_projected_latents_norm_stats[key][0]-sigma_threshold*self.pca_projected_latents_norm_stats[key][1])\n",
    "                          for key in self.pca_projected_latents_norm_stats.keys() }\n",
    "\n",
    "    def detect_l2_distance_ood(self, latent, timestep):\n",
    "        assert timestep in self.timesteps \n",
    "        projected_latent = latent @ self.pca_basis[timestep].T\n",
    "        norm = np.linalg.norm(projected_latent)\n",
    "        # print(norm)\n",
    "        # print(self.thresholds[timestep])\n",
    "        # l2_distances = np.linalg.norm(self.pca_projected_latents[timestep] - projected_latent, axis=1)\n",
    "        # min_distance = np.min(l2_distances)\n",
    "        if norm > self.thresholds[timestep][0] or norm < self.thresholds[timestep][1]:\n",
    "            print('ood sample!')\n",
    "            return True, norm\n",
    "        else:\n",
    "            print('id sample!')\n",
    "            return False, norm\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the OOD Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_norm_detector = pca_ood_detector_norm(record_latent_features)\n",
    "ood_norm_detector.flatten_features()\n",
    "ood_norm_detector.pca_analyze()\n",
    "ood_norm_detector.calc_avg_norm()\n",
    "ood_norm_detector.set_threshold(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the OOD Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# from unet import UNet\n",
    "from model import DiffusionModel\n",
    "from util import get_transforms, get_dataset, get_image_size, get_dataloader\n",
    "import sys\n",
    "from unet_power import UNet\n",
    "\n",
    "\n",
    "\n",
    "def infer(unet, diffusion_model, device, T, reverse_transform, n):\n",
    "\n",
    "    unet.train()\n",
    "    sd = []\n",
    "    total_samples = []\n",
    "    with torch.no_grad():\n",
    "        tqdmr = tqdm(range(n))\n",
    "        for _ in tqdmr:\n",
    "            samples = []\n",
    "            image = torch.randn((1, 1, 32, 32)).to(device)\n",
    "            for i in reversed(range(diffusion_model.timesteps)):\n",
    "                samples_at_step = []\n",
    "                for _ in range(T):\n",
    "                    image = diffusion_model.backward(image, torch.full((1, ), i, dtype=torch.long, device=device), unet)\n",
    "                    samples_at_step.append(image)\n",
    "                samples_at_step = torch.cat(samples_at_step, dim=0)\n",
    "                mean_sample = samples_at_step.mean(dim=0)\n",
    "                sd_sample = samples_at_step.std(dim=0).mean()\n",
    "                if i % 50 == 0:\n",
    "                    samples.append(reverse_transform(mean_sample).cpu())\n",
    "            sd.append(sd_sample.cpu().item())\n",
    "            total_samples.append(samples)\n",
    "    return total_samples, sd\n",
    "\n",
    "def infer_ood_abnormal(unet, diffusion_model, ood_detector, device, reverse_transform, n_imgs):\n",
    "\n",
    "    unet.eval()\n",
    "    unet._start_ood_detection(ood_detector, (0,199,399,599,799,999))\n",
    "    \n",
    "    infer_samples = []\n",
    "    ood_indicator = []\n",
    "    with torch.no_grad():\n",
    "        tqdmr = tqdm(range(n_imgs))\n",
    "        for _ in tqdmr:\n",
    "            samples = []\n",
    "            # image = torch.randint(0,255,(1, 1, 32, 32),dtype=torch.float).to(device)\n",
    "            image = torch.randn((1, 1, 32, 32)).to(device) * 1.5\n",
    "            for i in reversed(range(diffusion_model.timesteps)):\n",
    "                image = diffusion_model.backward(image, torch.full((1, ), i, dtype=torch.long, device=device), unet)\n",
    "                # print(i)\n",
    "                if i % 50 == 0:\n",
    "                    samples.append(reverse_transform(image).cpu())\n",
    "            infer_samples.append(samples)\n",
    "            if np.any([i[0] for i in unet.ood_detect_res[-5:-1]]):\n",
    "                ood_indicator.append('ood')\n",
    "            else:\n",
    "                ood_indicator.append('id')\n",
    "\n",
    "    return infer_samples, ood_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 1000\n",
    "IMAGE_SIZE = (32, 32)\n",
    "device = 'cuda'\n",
    "n_imgs = 5\n",
    "N = 50\n",
    "T = 2\n",
    "\n",
    "# ood_norm_detector.set_threshold(1.5) # previous success\n",
    "ood_norm_detector.set_threshold(2)\n",
    "unet = UNet(T=TIMESTEPS, ch=32, ch_mult=[1,2,2,2], attn=[1], num_res_blocks=2, \n",
    "                 dropout=0.1).to('cuda')\n",
    "unet.load_state_dict(torch.load('weight/parameters_power_mnist.pkl'))\n",
    "diffusion_model = DiffusionModel(timesteps=TIMESTEPS)\n",
    "_, reverse_transform = get_transforms(image_size=IMAGE_SIZE)\n",
    "\n",
    "infer_samples, ood_indicator = infer_ood_abnormal(unet, diffusion_model, ood_norm_detector, device, reverse_transform, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw ID/OOD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of 4 rows and 5 columns for subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 12))\n",
    "\n",
    "# Flatten the 2D array of subplots for easier indexing\n",
    "axes = axes.flatten()\n",
    "# Plot on each subplot\n",
    "for axes_idx,i in enumerate(np.array(range(50))[np.array(ood_indicator)=='ood'][0:]):\n",
    "    if axes_idx >= 20:  break\n",
    "    axes[axes_idx].imshow(infer_samples[i][-1][-1][0],cmap='gray',vmin=0, vmax=255)\n",
    "    axes[axes_idx].set_title(f'Plot {ood_indicator[i]}')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of 4 rows and 5 columns for subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 12))\n",
    "\n",
    "# Flatten the 2D array of subplots for easier indexing\n",
    "axes = axes.flatten()\n",
    "# Plot on each subplot\n",
    "for axes_idx,i in enumerate(np.array(range(50))[np.array(ood_indicator)=='ood'][0:]):\n",
    "    if axes_idx >= 20:  break\n",
    "    axes[axes_idx].imshow(infer_samples[i][-1][-1][0],cmap='gray',vmin=0, vmax=255)\n",
    "    axes[axes_idx].set_title(f'Plot {ood_indicator[i]}')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
